I am working on a hackathon project. I want to create a machine learning model
that hears an audio sample, then uses deep learning to infer exactly what
steps/knobs are required to reproduce that sound using a popular wavetable/fm
synth like Vital.

The goal is to create a project that you can record a sound, or hum a sound, and
it will be able to generate a step-by-step tutorial to make that sound in a
synth. LLMs can be used for the tutorial/intuitive understanding of the data.

Please research how I can implement this. Do a background research for all
existing works (research and github). Then, propose a model architecture,
implementation plan, and learning resources. Place an emphasis on advancing the
state of the art by using modern ML models used for audio, eg transformers,
diffusion, ppo, etc. I have a cuda gpu for training.

There's some really good notes available here:
https://gist.github.com/0xdevalias/5a06349b376d01b2a76ad27a86b08c1b
